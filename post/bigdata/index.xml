<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bigdata on mktkdyt</title>
    <link>https://mktkdyt.github.io/post/bigdata/</link>
    <description>Recent content in Bigdata on mktkdyt</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 14 Jan 2021 17:24:23 +0900</lastBuildDate><atom:link href="https://mktkdyt.github.io/post/bigdata/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pigで辞書を当てる</title>
      <link>https://mktkdyt.github.io/post/bigdata/pig%E3%81%A7%E8%BE%9E%E6%9B%B8%E3%82%92%E5%BD%93%E3%81%A6%E3%82%8B/</link>
      <pubDate>Thu, 14 Jan 2021 17:23:48 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/bigdata/pig%E3%81%A7%E8%BE%9E%E6%9B%B8%E3%82%92%E5%BD%93%E3%81%A6%E3%82%8B/</guid>
      <description>keywords.tsv
Hoge Fuga Piyo  辞書を読み込み，1行に変換する。  キーワードを OR で当てるため。    keywords_as_one_line = common_function.load_dictionary_as_one_line(&amp;#39;keywords.tsv&amp;#39;, &amp;#39;|&amp;#39;) def load_dictionary_as_one_line(dictionary_file, separator): with open(dictionary_file) as f: lines_one_str = &amp;#39;&amp;#39; # a\nb\nc\n -&amp;gt; a|b|c|d lines = f.readlines() for line in lines: word = line.rstrip(os.linesep) if(word != &amp;#39;&amp;#39;): lines_one_str += word + separator return lines_one_str[:-1]  本文に matches で辞書のキーワードが含まれるか見る。  Pig テンプレートに1行に変換したキーワードを埋め込む。    COND = { &amp;#39;$KEYWORDS&amp;#39;: f&amp;#39;.*({keywords_as_one_line}).*&amp;#39;, }_ common_function.translate_pig(&amp;#39;extract_text_template.pig&amp;#39;, &amp;#39;extract_text.pig&amp;#39;, COND) def translate_pig(template: str, output: str, d: {str, str}): for i, (k, v) in enumerate(d.</description>
    </item>
    
    <item>
      <title>Pig使い方まとめ</title>
      <link>https://mktkdyt.github.io/post/bigdata/pig%E4%BD%BF%E3%81%84%E6%96%B9%E3%81%BE%E3%81%A8%E3%82%81/</link>
      <pubDate>Thu, 14 Jan 2021 17:23:48 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/bigdata/pig%E4%BD%BF%E3%81%84%E6%96%B9%E3%81%BE%E3%81%A8%E3%82%81/</guid>
      <description>テキスト処理 改行除く  ^M（CR）も除く。  REPLACE(REPLACE(REPLACE(REPLACE(text, &amp;#39;\\t&amp;#39;, &amp;#39;&amp;#39;), &amp;#39;\\r\\n&amp;#39;, &amp;#39;&amp;#39;), &amp;#39;\\n&amp;#39;, &amp;#39;&amp;#39;), &amp;#39;\\\\^M&amp;#39;, &amp;#39;&amp;#39;) CONCAT  文字列を結合する。  CONCAT(&amp;#39;https://sample.jp?user=&amp;#39;, user) 条件演算 INDEXOF  指定した文字列がカラムに含まれているかを見る。  -- text に「プレゼント」が含まれていないレコードを抽出する FILTERED = FILTER DATA BY INDEXOF(text, &amp;#39;プレゼント&amp;#39;) &amp;lt; 0 -- text に「ビッグデータ」が含まれているレコードを抽出する FILTERED = FILTER DATA BY INDEXOF(text, &amp;#39;ビッグデータ&amp;#39;) &amp;gt; 0 日時フィルター  ToDate() で文字列を日時に変換可能。  -- 2019-05-31 00:00:00 から 2019-06-01 00:00:00 に登録されたユーザを抽出する FILTERED_USER = FILTER USER BY ToDate(&amp;#39;2019-05-31 00:00:00&amp;#39;,&amp;#39;yyyy-MM-dd HH:mm:ss&amp;#39;) &amp;lt;= ToDate(created,&amp;#39;yyyy-MM-dd HH:mm:ss&amp;#39;) AND ToDate(created,&amp;#39;yyyy-MM-dd HH:mm:ss&amp;#39;) &amp;lt; ToDate(&amp;#39;2019-06-01 00:00:00&amp;#39;,&amp;#39;yyyy-MM-dd HH:mm:ss&amp;#39;); 関係演算 ユニークを取る -- 重複を除く UNIQ_USER = DISTINCT USER; 取得件数を制限する -- 5 件に制限する L = LIMIT DATA 5; 集合演算 内部結合する USER1 = LOAD &amp;#39;/tmp/user1&amp;#39; USING PigStorage(&amp;#39;\\t&amp;#39;) AS (id:chararray); USER2 = LOAD &amp;#39;/tmp/user1&amp;#39; USING PigStorage(&amp;#39;\\t&amp;#39;) AS (id:chararray); J = JOIN USER1 BY id, USER2 BY id; 外部結合する USER1 = LOAD &amp;#39;/tmp/user1&amp;#39; USING PigStorage(&amp;#39;\\t&amp;#39;) AS (id:chararray); USER2 = LOAD &amp;#39;/tmp/user1&amp;#39; USING PigStorage(&amp;#39;\\t&amp;#39;) AS (id:chararray); J = JOIN USER1 BY id LEFT OUTER, USER2 BY id; 差集合を取る  JOIN して片方の Relation が NULL なレコードを抽出する。  USER1 = LOAD &amp;#39;/tmp/user1&amp;#39; USING PigStorage(&amp;#39;\\t&amp;#39;) AS (id:chararray); USER2 = LOAD &amp;#39;/tmp/user1&amp;#39; USING PigStorage(&amp;#39;\\t&amp;#39;) AS (id:chararray); J = JOIN USER1 BY id LEFT OUTER, USER2 BY id; USER1_MINUS_USER2 = FILTER J BY USER2::id is NULL; 積集合を取る  JOIN して片方の Relation が not NULL なレコードを抽出する。  USER1 = LOAD &amp;#39;/tmp/user1&amp;#39; USING PigStorage(&amp;#39;\\t&amp;#39;) AS (id:chararray); USER2 = LOAD &amp;#39;/tmp/user1&amp;#39; USING PigStorage(&amp;#39;\\t&amp;#39;) AS (id:chararray); J = JOIN USER1 BY id LEFT OUTER, USER2 BY id; USER1_AND_USER2 = FILTER J BY USER2::id is not NULL; 出力 TSV 出力 FS -rm -r -f -skipTrash /tmp/some_dir/ STORE DATA INTO &amp;#39;/tmp/some_dir&amp;#39; USING PigStorage(&amp;#39;\t&amp;#39;);  -schema オプションを付与することで load するときにカラム名と型を指定しなくてよくなる。  FS -rm -r -f -skipTrash /tmp/some_dir/ STORE DATA INTO &amp;#39;/tmp/some_dir&amp;#39; USING PigStorage(&amp;#39;\t&amp;#39;, &amp;#39;-schema&amp;#39;); ORC 出力  データ量を削減したい場合に使用する。  FS -rm -r -f -skipTrash /tmp/some_dir/ STORE DATA INTO &amp;#39;/tmp/some_dir&amp;#39; USING OrcStorage(&amp;#39;-c ZLIB&amp;#39;); Pig 実行 コマンド実行  Pig ファイルを指定し pig コマンドを実行する。  pig test.</description>
    </item>
    
    <item>
      <title>ビッグデータ分析で使用するPythonコード一覧</title>
      <link>https://mktkdyt.github.io/post/bigdata/%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8Bpython%E3%82%B3%E3%83%BC%E3%83%89%E4%B8%80%E8%A6%A7/</link>
      <pubDate>Thu, 14 Jan 2021 17:23:48 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/bigdata/%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8Bpython%E3%82%B3%E3%83%BC%E3%83%89%E4%B8%80%E8%A6%A7/</guid>
      <description>環境構築  必要に応じて仮想環境を作る。  仮想環境作成 $ python3 -m venv test 仮想環境有効化 $ source test/bin/activate (test)$ 仮想環境無効化 (test)$ deactivate $ 文字列処理 文字列を切り出す  [開始インデックス:終了文字番号(インデックスではないことに注意)]とする。  s = &amp;#34;2019-06-01&amp;#34; print(f&amp;#34;{s[0:4]}-{s[5:7]}-{s[8:10]}&amp;#34;) エスケープ 波括弧のエスケープ  波括弧は波括弧でエスケープする。  var = &amp;#39;aiuto&amp;#39; print( f&amp;#34;val is {{{var}}}&amp;#34; ) ディレクトリ操作 ディレクトリ作成 import os os.makidirs(&amp;#39;tmp&amp;#39;, exist_ok=True) class  プロパティ等を別ファイルにしたい場合等に使用する。  classsample ├── main.py └── prop └── user_property.py main.py
from prop.user_property import UserProperty user_property = UserProperty({&amp;#39;first_name&amp;#39;: &amp;#39;イチロー&amp;#39;, &amp;#39;family_name&amp;#39;: &amp;#39;テスト&amp;#39;}) print(f&amp;#39;{user_property.FAMILY_NAME} {user_property.</description>
    </item>
    
    <item>
      <title>ビッグデータ分析で使用するシェルコマンド一覧</title>
      <link>https://mktkdyt.github.io/post/bigdata/%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B%E3%82%B7%E3%82%A7%E3%83%AB%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E4%B8%80%E8%A6%A7/</link>
      <pubDate>Thu, 14 Jan 2021 17:23:48 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/bigdata/%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B%E3%82%B7%E3%82%A7%E3%83%AB%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E4%B8%80%E8%A6%A7/</guid>
      <description>ビッグデータ分析で使用してきたシェルコマンドをまとめた。
ターミナル表示 echo で文字列エスケープ $ echo &amp;#34;\&amp;#34;&amp;#34; &amp;#34; echo で改行表示  -e で改行を表示出来る。  $ txt=&amp;#34;a\nb\nc&amp;#34; $ echo -e &amp;#34;$txt&amp;#34; a b c テキスト処理 文字列から offset を指定して抽出する  日付文字列から年月日を抽出する際に使用する。  $ d=&amp;#34;20190630&amp;#34; $ echo ${d:0:4} 2019 $ echo ${d:4:2} 06 文字列の長さを取得する $ txt=&amp;#34;abcdef&amp;#34; $ echo ${#txt} 6 ファイル処理 tsvから特定の列を抽出する  cut コマンドで取り出せる。 ただし，複数列取り出せても順番を入れ替えることは出来ない。  $ cat test.tsv | cut -f3,4  awk でも可能。 awk だと列を入れ替えることができる。 -F で区切り文字を指定する  $ awk -F&amp;#39;\t&amp;#39; &amp;#39;{print $4 &amp;#34;\t&amp;#34; $3}&amp;#39; -f test.</description>
    </item>
    
    <item>
      <title>ビッグデータ分析のエラーハンドリング</title>
      <link>https://mktkdyt.github.io/post/bigdata/%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%AE%E3%82%A8%E3%83%A9%E3%83%BC%E3%83%8F%E3%83%B3%E3%83%89%E3%83%AA%E3%83%B3%E3%82%B0/</link>
      <pubDate>Thu, 14 Jan 2021 17:23:48 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/bigdata/%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%AE%E3%82%A8%E3%83%A9%E3%83%BC%E3%83%8F%E3%83%B3%E3%83%89%E3%83%AA%E3%83%B3%E3%82%B0/</guid>
      <description>リトライ時の処理スキップ  Pig 実行前に Pig の成果物が既に HDFS に存在するか確認し存在する場合は処理をスキップするようにした。 強制実行オプションを設け，指定された場合は成果物が既にある処理も実行されるようにした。  # 強制実行オプション @click.option( &amp;#39;--force&amp;#39;, &amp;#39;-f&amp;#39;, &amp;#39;force&amp;#39;, is_flag=True, help=&amp;#39;強制実行フラグ。これを立てたら既に抽出済みの処理も再実行する。&amp;#39; ) def exists_in_hdfs(hdfs_path: str) -&amp;gt; bool: &amp;#34;&amp;#34;&amp;#34;HDFS上のパスに指定したディレクトリ・ファイルが存在しているか否かを返す Arguments: hdfs_path {str} -- HDFS上のパス Returns: bool -- 存在していればTrue, していなければFalse &amp;#34;&amp;#34;&amp;#34; result = subprocess.run([&amp;#34;hadoop&amp;#34;, &amp;#34;fs&amp;#34;, &amp;#34;-test&amp;#34;, &amp;#34;-e&amp;#34;, hdfs_path]) if result.returncode == 0: return True else: return False def exists_all_in_hdfs(src_list) -&amp;gt; bool: &amp;#34;&amp;#34;&amp;#34;リストで指定したディレクトリ・ファイルが全て HDFS に存在するかを検証します。 Arguments: src_tupple -- HDFS のパス Returns: bool -- 全て存在すれば True，それ以外の場合は False &amp;#34;&amp;#34;&amp;#34; for src in src_list: if(not exists_in_hdfs(src)): logger.</description>
    </item>
    
    <item>
      <title>外部 API 並列処理</title>
      <link>https://mktkdyt.github.io/post/bigdata/%E5%A4%96%E9%83%A8api%E4%B8%A6%E5%88%97%E5%87%A6%E7%90%86/</link>
      <pubDate>Thu, 14 Jan 2021 17:23:48 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/bigdata/%E5%A4%96%E9%83%A8api%E4%B8%A6%E5%88%97%E5%87%A6%E7%90%86/</guid>
      <description>HDFS から対象のIDを取り出す。  uniq で重複排除する。   標準入力を split で n 分割する。  行数にデータ件数の 1/n を指定することで標準入力を n 分割する。 分割されたファイルは tmp/000* の形式で出力される（分割したらファイル出力されてしまう）。    split_cnt = 10 text_hdfs_dir = &amp;#39;/text&amp;#39; tmp_dir = &amp;#39;tmp&amp;#39; c = f&amp;#39;hadoop fs -cat \&amp;#39;{text_hdfs_dir}/part-*\&amp;#39;| cut -f1 | grep -E \&amp;#39;^[0-9]{{1,30}}$\&amp;#39;| sort | uniq | split -l $(($(hadoop fs -cat \&amp;#39;{text_hdfs_dir}/part-*\&amp;#39;| cut -f1 | grep -E \&amp;#39;^[0-9]{{1,30}}$\&amp;#39;| sort | uniq | wc -l)/{split_cnt}+1)) -d -a 5 - {tmp_dir}/&amp;#39; subprocess.</description>
    </item>
    
    <item>
      <title>Hadoopコマンドまとめ</title>
      <link>https://mktkdyt.github.io/post/bigdata/hadoop%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%81%BE%E3%81%A8%E3%82%81/</link>
      <pubDate>Thu, 14 Jan 2021 17:22:56 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/bigdata/hadoop%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%81%BE%E3%81%A8%E3%82%81/</guid>
      <description>Hadoop コマンドまとめ ジョブ関連 Hadoop のジョブを一覧表示する $ yarn application -list Hadoop のジョブを強制終了する $ yarn application -kill $application_id queue を変更する $ yarn application -movetoqueue $application_id -queue $queue_name HDFS ディレクトリ操作 HDFS のディレクトリの中身を確認する $ hadoop fs -ls /tmp/some_dir HDFS のディレクトリを削除する $ hadoop fs -rm -r /tmp/some_dir HDFS にディレクトリを作成する $ hadoop fs -mkdir /tmp/some_dir HDFS ファイル操作 HDFS にローカルのファイルを置く $ hadoop fs -put /home/user/some.tsv /tmp/some_dir/rawdata.tsv HDFS の中身を表示する $ hadoop fs -cat /tmp/some_dir/rawdata.tsv HDFS の中身をローカルに吐き出す HDFS に TSV で格納した場合。</description>
    </item>
    
  </channel>
</rss>
