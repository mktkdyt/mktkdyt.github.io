<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ログ分析 on mktkdyt</title>
    <link>https://mktkdyt.github.io/post/log_analysis/</link>
    <description>Recent content in ログ分析 on mktkdyt</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 14 Jan 2021 17:24:23 +0900</lastBuildDate><atom:link href="https://mktkdyt.github.io/post/log_analysis/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Elastic Stack 構築</title>
      <link>https://mktkdyt.github.io/post/log_analysis/elasticstack%E6%A7%8B%E7%AF%89/</link>
      <pubDate>Thu, 14 Jan 2021 17:24:23 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/log_analysis/elasticstack%E6%A7%8B%E7%AF%89/</guid>
      <description>Elastic Stack 構築 目次  各ソフトウェア概要 処理フロー Docker Compose で Elastic Stack を構築する  構成 docker-compose.yml .env logstash.conf filebeat.yml IIS ログファイルを置く Kibana で可視化する  Elasticsearch の Index 確認 Kibana の Index Pattern 作成 グラフ作成 ダッシュボード作成      各ソフトウェア概要  Elastic Stack は Elasticsearch，Kibana，Beats，Logstash からなるプロダクト群のことを指す。 Beats はデータシッパーと呼ばれ，データ転送ツールとして用いられる。  自動でファイルの更新を検知し差分を転送してくれる。 今回は Filebeats を使用する。   Logstash はデータ処パイプラインと呼ばれ，データを取り込み，変換し Elasticsearch に格納することが出来る。 Elasticsearch は言わずと知れた全文検索エンジン。データ投入時に転置インデックスを作成することで大量のドキュメントを高速に検索出来るようにしている。 Kibana は Elasticsearch のデータを可視化するツールとして用いられる。  処理フロー  処理フローは [Filebeat -&amp;gt; Logstash -&amp;gt; Elasticsearch -&amp;gt; Kibana] Filebeat で IIS ログを監視し，更新を検知したら Logstash に転送する。 Logstash で tsv を json に変換し，Elasticsearch に投入する。 Kibana で Elasticsearch のデータを可視化する。  Docker Compose で Elastic Stack を構築する Docker Compose で構築する。　docker-compose.</description>
    </item>
    
    <item>
      <title>ELK(Elasticsearch Kibana Logstash)構築</title>
      <link>https://mktkdyt.github.io/post/log_analysis/elkelasticsearchkibanalogstash%E6%A7%8B%E7%AF%89/</link>
      <pubDate>Thu, 14 Jan 2021 17:24:23 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/log_analysis/elkelasticsearchkibanalogstash%E6%A7%8B%E7%AF%89/</guid>
      <description>ELK(Elasticsearch Kibana Logstash)構築 目次  各ソフトウェア概要 Docker で ELK を構築する 動作確認  各ソフトウェア概要  処理フローは [Logstash -&amp;gt; Elasticsearch -&amp;gt; Kibana] Logstash はデータパイプライン。様々なソースからデータを収集，変換し目的の宛先に送信する。 Elasticsearch は全文検索エンジン。 Kibana はデータの可視化ツール。  Docker で ELK を構築する &amp;gt; docker-compose up -d.├─docker-compose.yml├─elasticsearch│ └─data└─logstash└─pipeline└─logstash.confdocker-compose.yml
version: &amp;#34;3&amp;#34; services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 environment: - discovery.type=single-node - cluster.name=docker-cluster - bootstrap.memory_lock=true - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34; ulimits: memlock: soft: -1 hard: -1 ports: - 9200:9200 volumes: - .</description>
    </item>
    
    <item>
      <title>Filebeat で IIS ログの更新を検知し Logstash で Elasticsearch に投入し Kibana で可視化する</title>
      <link>https://mktkdyt.github.io/post/log_analysis/filebeat%E3%81%A7iis%E3%83%AD%E3%82%B0%E3%81%AE%E6%9B%B4%E6%96%B0%E3%82%92%E6%A4%9C%E7%9F%A5%E3%81%97logstash%E3%81%A7elasticsearch%E3%81%AB%E6%8A%95%E5%85%A5%E3%81%97kibana%E3%81%A7%E5%8F%AF%E8%A6%96%E5%8C%96%E3%81%99%E3%82%8B/</link>
      <pubDate>Thu, 14 Jan 2021 17:24:23 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/log_analysis/filebeat%E3%81%A7iis%E3%83%AD%E3%82%B0%E3%81%AE%E6%9B%B4%E6%96%B0%E3%82%92%E6%A4%9C%E7%9F%A5%E3%81%97logstash%E3%81%A7elasticsearch%E3%81%AB%E6%8A%95%E5%85%A5%E3%81%97kibana%E3%81%A7%E5%8F%AF%E8%A6%96%E5%8C%96%E3%81%99%E3%82%8B/</guid>
      <description>Filebeat で IIS ログの更新を検知し Logstash で Elasticsearch に投入し Kibana で可視化する 目次  概要 Filebeat で IIS ログを収集し Logstash に渡す  Filebeat の設定ファイル修正 サービス起動   Logstash で Filebeat から IIS ログを受け取り加工し Elasticsearch に投入する  Logstash の設定ファイル修正 IIS ログファイルを置く   Kibana で可視化する  Elasticsearch の Index 確認 Kibana の Index Pattern 作成 グラフ作成    概要  Filebeat で IIS ログを監視し更新が発生したら Logstash に渡す。 Logstash で IIS ログを JSON に変換し Elasticsearch に投入する。 Kibana で Elasticsearch に格納された IIS　ログを可視化する。  Kibana ダッシュボード</description>
    </item>
    
    <item>
      <title>Filebeat インストール</title>
      <link>https://mktkdyt.github.io/post/log_analysis/filebeat%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB/</link>
      <pubDate>Thu, 14 Jan 2021 17:24:23 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/log_analysis/filebeat%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB/</guid>
      <description>Filebeat インストール 目次  アーカイブ取得 アーカイブ展開 サービスインストール 設定ファイル確認 サービス起動 フォアグラウンドで手動実行 サービスアンインストール  アーカイブ取得 https://www.elastic.co/jp/downloads/beats/filebeat から WINDOWS ZIP 64-BIT を取得する。
アーカイブ展開 [C:\Program Files] に Filebeat として展開する。
サービスインストール 管理者権限で PowerShell を起動し，アーカイブに含まれていたサービスをインストールするスクリプトを実行する。
&amp;gt; cd &#39;C:\Program Files\Filebeat\&#39;&amp;gt; .\install-service-filebeat.ps1Status Name DisplayName------ ---- -----------Stopped filebeat filebeat設定ファイル確認 &amp;gt; .\filebeat.exe -e test configサービス起動 &amp;gt; Start-Service filebeatフォアグラウンドで手動実行 &amp;gt; .\filebeat.exe -c filebeat.yml -e -d &amp;quot;*&amp;quot;サービスアンインストール 管理者権限で PowerShell を起動し，アーカイブに含まれていたサービスをアンインストールするスクリプトを実行する。
&amp;gt; .\uninstall-service-filebeat.ps1</description>
    </item>
    
    <item>
      <title>Kibana のダッシューボードを Excel に出力する</title>
      <link>https://mktkdyt.github.io/post/log_analysis/kibana%E3%81%AE%E3%83%80%E3%83%83%E3%82%B7%E3%83%A5%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%82%92excel%E3%81%AB%E5%87%BA%E5%8A%9B%E3%81%99%E3%82%8B/</link>
      <pubDate>Thu, 14 Jan 2021 17:24:23 +0900</pubDate>
      
      <guid>https://mktkdyt.github.io/post/log_analysis/kibana%E3%81%AE%E3%83%80%E3%83%83%E3%82%B7%E3%83%A5%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%82%92excel%E3%81%AB%E5%87%BA%E5%8A%9B%E3%81%99%E3%82%8B/</guid>
      <description>Kibana のダッシューボードを Excel に出力する Kibana のダッシュボードを Excel に出力する機能がないのでスクリプトで出力出来るようにした。
概要  Kibana のグラフは一系列のものであれば CSV 出力することは出来るが，複数系列のもの（Timelion）は出力出来ない。 Kibana は Elasticsearch のデータを可視化しているので，Elasticsearch から直接データを持ってきて Excel に変換すれば自動化出来る。  手順 &amp;gt; wsl $ ./export.sh &amp;#34;2021-01-12T23:45:00.000Z&amp;#34; &amp;#34;2021-01-13T08:30:00.000Z&amp;#34; &amp;#34;20210113&amp;#34; $ ./generate_graph.py &amp;#34;20210113&amp;#34; Elasticsearch からデータを CSV 出力する  curl で Elasticsearch から集計結果を JSON で取得する jq で JSON を csv に変換する 系列が複数ある場合は awk | paste で CSV をマージする  ####################################### # Elasticsearch から Grafana で表示しているデータを抽出し CSV に出力する。 # Globals: # H: Elasticsearch に送信する HTTP リクエストの Content-Type。 # U: Elasticsaerch の URL。 # TIME_HHMM_REGEX: Elasticsearch のレスポンスから時間のみを抜き出す正規表現。 # Arguments: # $1: 検索対象開始日時。日本標準時の9時間前を指定すること。（例：2020-12-13T23:45:00.</description>
    </item>
    
  </channel>
</rss>
